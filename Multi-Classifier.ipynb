{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0e75d9-95d4-4f29-b551-0ad0e4120934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c80d3d82-79d4-407a-ae40-3843b54934d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (for preprocessing purposes)\n",
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ab69dc-d85b-4258-962c-a0dc77403f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and preprocessing steps as used in the original script\n",
    "X = data.iloc[:, :-1]\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc0c5ab-b11e-4d41-b67e-ce2add610d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessor: OneHotEncoder for categorical and StandardScaler for numeric features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "        ('num', StandardScaler(), X.select_dtypes(include=['int64', 'float64']).columns)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "240cae35-25f9-43e0-8afd-f0e58a022edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models have been pickled successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Assume these are your trained models from earlier\n",
    "svm_model = SVC(kernel='rbf', probability=True)  # Replace with your trained model\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100)  # Replace with your trained model\n",
    "gradient_boosting_model = GradientBoostingClassifier(n_estimators=100)  # Replace with your trained model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)  # Replace with your trained model\n",
    "neural_network_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)  # Replace with your trained model\n",
    "\n",
    "# Save the models to disk\n",
    "joblib.dump(svm_model, 'svm_model.pkl')\n",
    "joblib.dump(random_forest_model, 'random_forest_model.pkl')\n",
    "joblib.dump(gradient_boosting_model, 'gradient_boosting_model.pkl')\n",
    "joblib.dump(knn_model, 'knn_model.pkl')\n",
    "joblib.dump(neural_network_model, 'neural_network_model.pkl')\n",
    "\n",
    "print(\"Models have been pickled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c08d8d-3714-4ef2-ba05-6be0ef9f777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of models and their descriptions\n",
    "models = {\n",
    "    '1': ('SVM', joblib.load('svm_model.pkl')),\n",
    "    '2': ('Random Forest', joblib.load('random_forest_model.pkl')),\n",
    "    '3': ('Gradient Boosting', joblib.load('gradient_boosting_model.pkl')),\n",
    "    '4': ('k-NN', joblib.load('knn_model.pkl')),\n",
    "    '5': ('Neural Network', joblib.load('neural_network_model.pkl'))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0a3f251-4b75-48e2-a396-4c0fa89a9ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter value for EducationSector:  3\n",
      "Enter value for IndividualProject:  3\n",
      "Enter value for Age:  4\n",
      "Enter value for Gender:  2\n",
      "Enter value for City:  4\n",
      "Enter value for Influenced:  5\n",
      "Enter value for Perseverance:  1\n",
      "Enter value for DesireToTakeInitiative:  4\n",
      "Enter value for Competitiveness:  5\n",
      "Enter value for SelfReliance:  5\n",
      "Enter value for StrongNeedToAchieve:  3\n",
      "Enter value for SelfConfidence:  1\n",
      "Enter value for GoodPhysicalHealth:  2\n",
      "Enter value for MentalDisorder:  3\n",
      "Enter value for KeyTraits:  4\n",
      "Enter value for ReasonsForLack:  5\n"
     ]
    }
   ],
   "source": [
    "# Get user input for the features\n",
    "input_data = {}\n",
    "for col in X.columns:\n",
    "    user_input = input(f\"Enter value for {col}: \")\n",
    "    if col in categorical_columns:\n",
    "        input_data[col] = user_input\n",
    "    else:\n",
    "        input_data[col] = float(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "947b61d2-2fe8-40d8-a79e-ebc0a575960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input to a DataFrame\n",
    "input_df = pd.DataFrame([input_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23c1bef1-feba-4a66-9803-a0b3bb42d1a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess the input data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m input_processed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Allow the user to choose a model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChoose a model to predict your likelihood of success:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1035\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform X separately by each transformer, concatenate results.\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \n\u001b[0;32m   1011\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;124;03m    sparse matrices.\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m _raise_for_params(params, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1035\u001b[0m \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1036\u001b[0m X \u001b[38;5;241m=\u001b[39m _check_X(X)\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m# If ColumnTransformer is fit using a dataframe, and now a dataframe is\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m# passed to be transformed, we select columns by name instead. This\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m# enables the user to pass X at transform time with extra columns which\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;66;03m# were not present in fit time, and the order of the columns doesn't\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m# matter.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1661\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Preprocess the input data\n",
    "input_processed = preprocessor.transform(input_df)\n",
    "\n",
    "# Allow the user to choose a model\n",
    "print(\"\\nChoose a model to predict your likelihood of success:\")\n",
    "for key, (name, _) in models.items():\n",
    "    print(f\"{key}: {name}\")\n",
    "\n",
    "selected_model_key = input(\"Enter the number corresponding to the model you want to use: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf88608-b5be-4591-888e-b1692c734e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chosen model\n",
    "if selected_model_key in models:\n",
    "    model_name, model = models[selected_model_key]\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(input_processed)\n",
    "    probability = model.predict_proba(input_processed)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"\\nUsing the {model_name} model:\")\n",
    "    if probability is not None:\n",
    "        print(f\"Likelihood of success: {probability[0] * 100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Predicted outcome: {prediction[0]}\")\n",
    "else:\n",
    "    print(\"Invalid model selection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
